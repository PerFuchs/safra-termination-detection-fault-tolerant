\section{Discussion}
\label{sec:discussion}
First, this section concludes what the results imply about SafraFT. Next, I discuss limitations and improvements of my approach.

An important aim of the experiment is to show the correctness of SafraFT.
Towards this aim, the results leave no doubt that SafraFT behaved correctly in all 2149 %NUMBER
runs.
These runs cover a wide range of different environments from small networks of 50 nodes to big networks with 2000 instances and for each network size the fault-free case, as well as two quite different fault scenarios.
The basic algorithm is carefully chosen to exhibit an interesting and varied message pattern in terms of the number of messages and communication partners.
Furthermore, Chandy-Misra leads to many changes between the active and the passive state of each node.
The events on which to simulate node failure are deliberately chosen to put high pressure on the ability of SafraFT to recover faults and should trigger many different executions.
All in all, the situations tested are designed to test SafraFT thoroughly under stress.
Still, termination is detected in every case; with no case of early detection.

However, the experiment shows that the definition of termination used to develop SafraFT, although very common and widely accepted, does not cover all situations arising in practice.
The definition assumes passive nodes only become active when they receive a basic message.
In reality, nodes can also change to an active state when they detect the crash of another node and need to take corrective actions.
When I realized this, I extended the definition of termination as described in~\cref{extended-definition} and evaluated the results of the experiment according to the original definition and the extension.
As stated above, SafraFT does not detect termination early according to the original definition.
When the extended version is used, SafraFT detects early termination in only 14 out of 1124 runs with crashes. % NUMBER
Such cases are unlikely in praxis because it is less likely there than in our experiment that a fault happens just before termination, because in our experiment many faults are triggered when a token is forwarded or received while in praxis most faults ought to happen within the basic algorithm or idle time because the vast majority of the time is spent for these purposes.
Furthermore, one can actively reduce the chance of early termination detection by using a failure detector that propagates failures fast to all nodes and then enforces an extra token round.
Finally, this is a general problem of termination detection in the presence of faults:
when a basic algorithm can be activated by detecting a fault, this can happen directly before actual termination but then there is inevitably a short period of time in which termination can be announced because detection of the crash  and propagation of this information cannot be instantly.

The practical comparison between SafraFT and SafraFS shows that there is no change in message complexity and confirmed the hypothesis that bit complexity raises linearly with the network size.
This higher bit complexity is the reason why SafraFT takes longer to detect termination after actual termination and leads to a higher processing time overhead over the basic algorithm than for SafraFS.
This processing time overhead is higher than we expected.

A possible explanation can be found in my experiment setup: each physical node simulates between 50 and 100 instances.
Each of this instances uses multiple threads; altogether there are at least 4 times as many threads as cores on each machine.
This leads to threads being preempted by the operating system (its stopped and control is given to another thread, e.g. one of another instance) which happens more often for threads that try to send big messages.
This theory is supported by the fact that the effect is stronger for networks with 2000 nodes.
However, experimental confirmation of this hypothesis could not be gained within the time limits of this project.

I would like to point out that my experimental setup imposes that the time taken by SafraFT to detect termination is long.
This is because Chandy-Misra and Afek-Kutten-Yung complete their tasks in a few seconds and then SafraFT needs roughly the same time to detect termination.
However, if I had chosen a basic algorithm that takes multiple hours to complete, the seconds taken by SafraFT to detect termination would be seen in a different perspective.
Furthermore, the low processing time overhead (less than 13\% in all runs of this experiment)  % NUMBER
of SafraFT plays a bigger role in long-running jobs.

Towards the performance of SafraFT in the presence of faults, I conclude that the number of backup tokens issued is not limited by the number of faults, but the average is lower than the number of faults.
Also, the increase in bytes per token is only constant (5 to 8 bytes) for a few faults and increases by 1.29 times for 90\% node failure.  % NUMBER
It is difficult to predict general relationships between the number of faults, network size and other measured metrics, e.g. time or token sent.
Still, the experiments clearly show that even in big networks and with 90\% node failure, the algorithm performs well and that none of the metrics shows exponential growth.
An especially interesting metric is the number of tokens sent in faulty networks: it shows relative independence from the number of failing nodes.
For both fault scenarios of five or fewer failing nodes and 90\% node failure on all network sizes, at most 3 times as many tokens were sent as for a fault free run. %NUMBER.
This indicates that independent from the number of faults, one more token round is necessary to detect termination.
To conclude, SafraFT handles faults in a sound and kind manner.

In conclusion, the experiments present strong evidence towards correctness of SafraFT, give a realistic comparison between the fault-tolerant version and the original version of Safra and demonstrate SafraFT's ability to handle faults.
In the next paragraphs, I point out possible improvements for this project.

We would have liked to run repetitions with even bigger networks than 2000 nodes.
This was hindered by the limited physical resources at hand - only 42 nodes with 8 cores each.
Therefore, even for 2000 instance multiple of these had to run in parallel on the same core.
That leads to slow runs for big networks and possibly unclear timing metrics.

The situation could be improved by:
\begin{itemize}
    \item More hardware.
    \item More efficient use of the existing hardware, e.g. a more efficient implementation of the \co{MessageUpcall} class by using a producer/consumer scheme overall messages to greatly reduce the number of active threads.
    \item Compromising between a realistic setup and an efficient setup, e.g. by using only one Java and Ibis instance per physical node and different threads for all instances on that node, instead of processes.
\end{itemize}


IPL does not provide rather important primitives as broadcasting or barriers.
So both had to be implemented by me during the project.
Especially, implementing robust and performant barriers showed to be more work than expected, as one IPL feature (signals) breaks with 2000 nodes and it demonstrated to be impossible to connect each node to one master node to orchestrate actions.
Hence, I recommend checking if a messaging library with a more comprehensive library exists, e.g. MPI or Akka\footnote{An implementation of the Actor model for the JVM written in Scala}.

With regardsd to showing the general relationship between network size and behaviour in the presence of faults is not the main goal of this project, I had hoped to come to more precise conclusions regarding this matter.
To do so one would need to run the experiments on bigger networks and preferably concentrate on a specific but relevant fault scenario.
Even with this additional data, it could prove very hard to attain general conclusions as faults do not only influence SafraFS but the whole system, e.g. a fault always leads to fewer nodes in the system and therefore changing the network size, or it leads to different behaviour of the basic algorithm which in term changes Safra's activity.

Finally, this project leaves the open question: how to define termination in the presence of faults?
The commonly used definition seems to be fundamentally flawed by the fact that most fault-tolerant algorithms need to react to a fault.
Therefore, they become active when they detect a fault.
This is not trivially fixed because a fault can happen at any time and is not predictable.
An interesting starting point for further research on this topic can be found in the literature about self-stabilizing algorithms, which use the notion of \textit{final fault}.
Note that neither the extended definition, which I proposed, nor the notion of \textit{final fault} are practical solutions because they can only be checked after execution by offline analysis.