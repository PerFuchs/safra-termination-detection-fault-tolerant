\section{Discussion}
First, this section concludes what the results imply about SafraFT. After, I discuss limitations and improvements of my approach.

The most important aim of the experiment is to show correctnes of SafraFT.
Towards this aim, the results leave no doubt that SafraFT behaved correctly in all 1500 %NUMBER
runs.
These runs cover a wide range of different environments from small networks of 50 nodes to big networks with 2000 instances and for each network size the fault-free case, as well as, two quite different fault scenarios.
The basic algorithm is carefully chosen to exhibit an interesting and varied message pattern in terms of the number of messages and communication partners.
Furthermore, Chandy Misra leads to many changes between the active and the passive state of each node.
The events on which to simulate node failure are deliberately chosen to put high pressure on SafraFT ability to recover faults and should trigger many different executions.
All in all, the situations tested are designed to test SafraFT thoroughly under stress.
Still, termination is detected in every case with no case of early detection.

However, the experiment shows that the definition of termination used to develop SafraFT, although very common and widely accepted, does not cover all situations arising in practice.
The definition assumes passive nodes only become active when they receive a basic message.
In reality, nodes can also change to an active state when they detect the crash of another node and need to take corrective actions.
When I realized this, I extended the definition of termination as described in \label{extended-definition} and evaluated the results of the experiment according to the original definition and the extension.
As stated above, SafraFT does not detect termination early according to the original definition.
When the extended version is used SafraFT detects early termination in only (\OutputFileNum{figures/early-termination.txt} out of 1000 runs and in only NUMBER cases this leads to a corrupted result. % NUMBER
Such cases are unlikely in praxis because it is less likely there than in our experiment that a fault happens just before termination because in our experiment many faults are triggered when a token is forwarded or received while in praxis most fault ought to happen within the basic algorithm or idle time because the vast majority of the time is spent for these purposes.
Furthermore, one can actively reduce the chance of early termination detection by using a failure detector that propagates failures fast to all nodes and then enforces an extra token round.


The practical comparison between SafraFT and SafraFS shows that there is no change in message complexity and confirmed the hypothesis that bit complexity raises linearly with the network size. 
This higher bit complexity is the reason why SafraFT takes longer to detect termination after actual termination and leads to a higher processing time overhead over the basic algorithm than for SafraFS.
I would like to point out that my experimental setup imposes that the time taken by SafraFT to detect termination is long.
This is because Chandy Misra and Afek-Kutten-Yung complete their tasks in in a few seconds and then SafraFT needs roughly the same time to detect termination.
However, if I had chosen a basic algorithm that takes multiple hours to complete, the seconds taken by SafraFT to detect termination would be seen in a different perspective.
Furthermore, the low processing time overhead (less than TODO% TODO \OutputFileNum{figures/less-than-processing-time-overhead.txt}\%)  % NUMBER
of SafraFT plays a bigger role in long-running jobs.

Towards SafraFT performance in the presence of faults, I conclude that the number of backup tokens issued is not limited by the number of faults but the average is lower than the number of faults.
Also, the increase in bytes per token is only constant (5 to 9 bytes) for a few faults and grows only linearly with the network size for 90\% node failure.  % NUMBER
It is difficult to predict general relationships between the number of faults, network size and other measured metrics e.g. time or token sent.  
Still, the experiments clearly show that even in big networks and with 90\% node failure the algorithm performs well and none of the metrics show exponential growth.
An especially interesting metric is the number of tokens sent in faulty networks: it shows relative independence from the number of failing nodes.
For both fault scenarios of five or less failing nodes and 90\% node failure on all network sizes the biggest increase in token sent over a fault free execution is 2.13 times. %NUMBER.
This indicates that independent from the number of faults one more token round is necessary to detect termination.
To conclude, SafraFT handles faults in a sound and kind manner.

In total, the experiment reaches its goals, it presented strong evidence towards correctness of SafraFT, gives a realistic comparison between the fault tolerant version and the original version of Safra and demonstrates SafraFT's ability to handle faults.
In the next paragraphs, I point out possible improvements for this project.

We would have liked to run repetitions with even bigger networks than 2000 nodes.
This was hindered by the limited physical resources at hand - only 42 nodes with 8 cores each.
Therefore, even for 2000 instance multiple of these had to run in parallel on the same core. 
That leads to slow runs for big networks and possibly unclear timing metrics.

The situation could be improved by:
\begin{itemize}
    \item More hardware
    \item More efficient use of the existing hardware e.g. a more efficient implementation of the \co{MessageUpcall} class by using a producer/consumer scheme over all messages to greatly reduce the number of active threads
    \item Compromising between a realistic setup and a efficient setup e.g. by using only one Java and Ibis instance per physical node and different threads for all instances on that node, instead of processes
\end{itemize}


IPL does not provide rather important primitives as broadcasting or barriers.
So both had to be implemented by me during the project.
Especially, implementing robust and performant barriers showed to be more work than expected as one IPL feature (signals) breaks with 2000 nodes and it demonstrated to be impossible to connect each node to one master node to orchestrate actions.
Hence, I recommend to check if a messaging library with a more comprehensive library excist e.g. MPI or Akka~\footnote{An implementation of the Actor model for the JVM written in Scala}.

Although showing the general relationship between network size and behaviour in the presence of faults is not the main goal of this project, I hoped to come to more precise conclusions regarding this matter.
To do so one would need to run the experiments on bigger networks and preferably concentrate on a specific but relevant fault scenario.
Even with this additional data, it could prove very hard to attain general conclusions as faults do not only influence SafraFS but the whole system e.g. a fault always leads to fewer nodes in the system and therefore changing the network size or it leads to different behaviour of the basic algorithm which in term changes Safra's activity.

Finally, this project leaves the open question: how to define termination in the presence of faults?
The commonly used definition seems to be fundamentally flawed by the fact that most fault tolerant algorithm need to react to a fault.
Therefore, they become active when they detect a fault.
This is not trivially fixed because a fault can happen at any time and is not predictable.
An interesting starting point for further research on this topic can be found in the literature about self-stabilizing algorithms which uses the notion of \texit{final fault}.
Note, that the extended definition, which I proposed, nor the notion of \textit{final fault} are practical solutions because they can only be checked after execution by offline analysis.