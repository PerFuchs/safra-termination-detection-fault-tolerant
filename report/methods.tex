\section{Methods}  % TODO good title?

\subsection{Chandy Misra}
 - src Distributed algorithms by Fokking
 - either write and connec this ro just reference this in the next chapter
\subsection {Fault tolerant Chandy Misra}
  The fault tolerant Chandy Misra version used for our experiments constructs a sink tree in an undirected network under the assumption that a perfect failure detector is present at each node (a detector that does not suspect nodes that haven't actually failed and also will detect each failure eventually). 
  Other than, the original Chandy Misra algorithm this version requires FIFO-channels.
  Furthermore, I assume that the root node cannot fail because otherwise there is no sink tree to construct.
  
  As far as Chandy-Misra is concerned nodes are only interested in crashes of their parents and other ancessor on their path to the root node.
  If a node \c{X} detects a crash of its parent, it sends a \c{REQUEST} message to each neighbour. 
  If a neighbour \c{Y} of \c{X} receives a \c{REQUEST} message, it answers with a \c{DIST d} messages where \c{d} is its own distance. 
  To save message \c{DIST d} is only send if $d < \infty $.
  If \c{Y} happens to be a child of \c{X}, it resets its own \c{dist} and \c{parent} value to $\infty$ respectively $\bot$ and sends a \c{REQUEST} message to all its neighbours.
  
  I argue that this augmented Chandy Misra algorithm constructs correct sink trees in presence of fail-safe failures. 
  Each failure only affects nodes that see themselves as children, grandchildren and so on; that is, it only affects subtrees.
  Because the perfect failure detector guarantees that each node failure will eventually be detected at the children of the failed node, they eventually send \c{REQUEST} message to all their neighbours.
  The neighbours send \c{REQUEST} message to all their neighbours if they receive a \c{REQUEST} message from their parent. 
  Therefore, eventually all nodes in the subtrees of a failing node are reached by \c{REQUEST} message and reset their \c{dist} and \c{parent} values.
  Also all neighbours that receive a \c{REQUEST} message of a node that is not their parent, answer the \c{REQUEST} message with their current \c{dist} value.
  This allows nodes in the affected subtree to rebuild new paths toward the root node.
  These new paths are correct when the answering node is not part of any affected subtree.
  However, if they are part of an affected subtree (e.g. grand children of the crashed nodes), invalid paths are introduced - as these nodes might not be reached by any \c{REQUEST} message and therefore still believe they have a valid path towards root.
  These invalid paths are corrected when the grandchildren are reached by the \c{REQUEST} message of their parent because on receipt they send \c{REQUEST} messages which reset all nodes considering them
  parents. 
  This possible behaviour of introducing invalid paths that are corrected later, might lead to a bad theoretical message complexity but did not hinder the experiments.
  This indicates that this behaviour is not often triggered in praxis.
  
  The presented fault tolerant Chandy Misra algorithm could be improved by relieving the necessity for FIFO-channels, a more formal proof of correctness and a thourough complexity analysis.

  Anyhow, my work suggests that the Chandy Misra version is correct because I ran it more than % TODO 
  times and compared the constructed trees to the expected trees which where determined offline. The algorithm yielded the correct sink tree every time.
  
  
 %TODO Provide pseudo code?
  
\subsection{Fault Simulation}
	I simulate faults by stopping all Safra and basic algorithm processing, as well as, Safra or basic message sending in the faulty instance. Faults are triggered before and after interesting events e.g. directly before or after sending a basic message or token. 
	Before every experiment run it is determined at random which node is going to fail, on which event it is going to fail and after how many repetitions of this event e.g. after the 3rd time it forwards a token.
	
	In particular, I selected the following events to trigger a crash if not specified differently the crash is triggered before or after the event:
	\begin{itemize}
	   	\item sending a token (1 to 3 repetitions)
	   	\item sending a backup token (1 to 2 repetitions)
	   	\item before receiving a token (1 to 3 repetitions)
	   	\item sending a basic message (1 to 5 repetitions)
	\end{itemize}
	The range of repetitions is limited to maximize the chance that a node meant to fail actually does so. 
	However, a node that is planned to fail is not guaranteed to do so.
	I verify for every experiment that a reasonable amount of nodes have failed with regard to the planned amount.
	
	Alternatively, to the chosen approach to trigger failures, I considered the more random mechanism of running a thread that kills an instance after a random amount of time.
	One could argue that this would be more realistic.
	However, I believe this kind of a approach leads to less interesting failures because the vast majority of these failure would occure during idle time. 
	Furthermore, most failures between internal events are observed as exactly the same on all other nodes. 
	For example, other nodes cannot observe if a failure happened before an internal variable changed or after. 
	In fact, they can only observe a difference when the failure happens after or before sending a message to them.
	Hence, I have chosen a fault mechanism that focusses on these distinguishable scenarios.
	As one might notices, the failure points are chosen to give raise to many different situations for our Safra versio to deal with. 
	I deliberately decided against chosing special failure points with regard to the basic algorithm because this would be lead to less focussed testing of the fault tolerance of Safra.

\subsection{Fault Detection}
\subsection{Offline Analysis}

\subsection{Enviroment}
\subsubsection{IBIS}
\subsubsection{DAS 4}
\subsubsection{Network Topology}