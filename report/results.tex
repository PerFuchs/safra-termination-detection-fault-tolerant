\section{Results}
\label{sec:results}
Over the next sections I present the main results of my experiment to support our claim that SafraFT is correct, to show how SafraFT compares to SafraFS and to exemplify the performance of SafraFT under the presence of faults.

The observations are based on runs of the system described in \cref{sec:methods} on the DAS-4 cluster at the Vrije Universiteit of Amsterdam.
I meassured runs on networks from 50 to 2000 nodes for SafraFT and SafraFS.
SafraFT was also tested with 1 to 5 nodes failing per run (dubbed 5n) and with 90\% node failure.
\cref{table:runs} presents how many repetition of each configuration were run.

The raw data including a manual how to interpret it can be found TODO here.
\begin{table}[]
	\begin{tabular}{@{}llll@{}}
		\toprule
		Algorithm & Network              & Faults & Repetitions  \\ \midrule
		SafraFS   & 50/250/500/1000/2000 & 0      & 40 each TODO \\
		SafraFT   & 50/250/500/1000/2000 & 0      & TODO         \\
		SafraFT   & 50/250/500/1000/2000 & 5n     & TODO         \\
		SafraFT   & 50/250/500/1000/2000 & 90\%   & TODO         \\ \bottomrule
	\end{tabular}
	\label{table:runs}
	\caption{List of all configurations run.}
\end{table}

\subsection{Correctness of SafraFT}
The experiment is aimed to support our paper with a practical, correct application of our alogorithm.
Towards this goal I build multiple correctness checks into the experiment. 

To assure nothing happens after termination detection, the application logs if any messages are received or actions are executed after termination has been detected and announced. 
The analysis tools provided with the experiment point these logs out to make sure they are not overlooked.

To proof termination is not detected to early, I use offline analysis (see \cref{ssec:offline-analysis}) to determine the point of actual termination and verify that detection happened after.

The experiment revealed a problem with the framework for termination chosen to develop SafraFT.
It is common to define termination by:
\begin{enumerate}
	\item All nodes are passive
	\item No messages are in the channels
\end{enumerate}
This definition is based on the fact that a node is either an initiator or can only become active if it receives a message. 
However, under the presence of failures and if additionally the outcome of the algorithm depends on the set of alive nodes, nodes might get activated by the detection of a failure.
For example, when Chandy Misra builds a sink tree, nodes that detect a crash of their parents will become active afterwards to find a new path towards root.
Hence, the definition of termination that our algorithm is build upon does not fully capture our choice of basic algorithm which will lead to an early detection of termination.

The fact described above leads to the following concrete scenario: lets consider the situation that all node are passive and no message are in the channel. 
In other words, the system terminated by our definition.
Node \co{X} forwards the token and crashes afterwards. 
Node \co{Y} calls announce after receipt of the token.
Assume node \co{Z} is a child of \co{X} and detects the crash of its parent, it becomes active after termination has been formally reached and announced.
By sending out \co{REQUEST} message, it might activate other nodes again.

In %TODO 
out of %TODO 
runs scenarios like the one described above were encountered. 
These runs might indicate that announce was called to early or and that messages were received after termination.
When this happens the run also provides further information about the announce event, all crash and all parent crash detection events in its logs to ease verification that the errors were caused by a scenario as described above.
Each of these run was carefully examined and if the errors were caused by this known problem, they were declared invalid and not used for further analysis.

compare expected tree to actual tree
* explain problem with CM about nodes that should fail not failing
* check every case of problems with CM

% TODO add total timer after termination to metrics in introdution  
token bytes constant 12 for SafraFT linear upwards for SafraFT starting at 420 and endind at 

\subsection{Comparision of Safra versions}
This section compares SafraFS and SafraFT. 
Additionally, it analysis how the network size influences both algorithms.

The number of tokens send in total and after termination is presented in \cref{fig:tokens.png}.
The key observation is that SafraFS and SafraFT behave higly similiar.
There is no notable difference between the medians, the variance and the ratio between total tokens and tokens after termination for the same network size - except for a unusual high variance in tokens after termination SafraFS for 1000 nodes and a high variance in total tokens for SafraFS for 2000 nodes % TODO is that a dirty experiment - can it be fixed by repeting it?
As one would expect, the number of tokens seems to grow linearly with the network size.
Note that the first network size is 5 times smaller than the second after the network size doubles for each run.

The bit complexity of SafraFS is constant.
In this experiment each token of SafraFS contains 12 bytes.
SafraFT has a bit complexity linearly to the network size (when no faults occur).
For a network of 50 nodes each token has 420 bytes; a token in a 2000 node network counts 16020 bytes.
The growth can be described by $bytes = 8 * <network size> + 20$.

I measured two kinds of timing metrics in this experiment.
On the one hand, there are the wall time metrics of total time and total time after termination.
Both were recorded in elapsed seconds between two events. 
These events are start of the Safra and basic algorithm until each instance is informed of termination for total time. 
Total time after termination is defined as the ammount of seconds between the actual termination (as defined in \cref{TODO}) and the event of an node calling
announce. % TODO is that too much explanation, does everybody but you know so?
On the ohter hand, there are basic, Safra and Safra after termination processing times (including the time needed to send of messages).
These are the accumulated times all instances needed to process basic or Safra functionality.
Total times and processing times are measured in a different way and should not be compared directly for multiple reasons. 
First, while total times include idle times, time spent for logging (where the process did not execute or methods), processing time do not include these.
Secondly, total time is wall time between two events and processing times are accumulated over all processes. 
One particular example for when this leads to differences is that time spent concurrently by two processes counts double in processing time metrics but only once in wall time metrics.

One can observer in \cref{table:processing-times} that SafraFT uses much more processing time than SafraFS and most of this time is spent between actual terminatio and termination detection.
Furthermore, one sees that while SafraFS timing grow much slower than SafraFT timings and that the difference becomes bigger. 
This hints for a change in time complexity between the two versions.

A small subexperiment of excluding the time spent to send messages from the processing time revealed that most of this difference can be tributed to writting tokens onto the wire. % TODO wording
As we know from the previous section, the number of token send does not differ between the algorithms.
Therefore, I believe that these differences are caused by the higher bit complexity of SafraFT. 
This would explain the total increase in the timing from SafraFS to SafraFT, as well as, the change of time complexity.
The time complexity would change because the increase in network size leads to more token being sent (as in SafraFS) but also to bigger tokens being sent.

Most of the additional time for SafraFT is spent after termination.
By the hypothesis introduced in the last paragraph, this is explained by most tokens being sent after termination (see \cref{fig:tokens}).

The processing time \cref{table:processing-times} also presents a comparision of the time spent for the basic algorithm and both Safra versions.
Although SafraFT uses significantly more time, the overhead on the processing time stays moderate.

The same pattern of SafraFT using more time and reacting worse to an increase in network size is visible for total times in \cref{table:total-times}.
Again the majority of this time is spent after actual termination for the same reason as before.
I would like to note that the low processing time overhead of Safra is not in contradiction to the large amount of wall time spent after termination.
This seemingly opposing results arise from the difference between wall time and processing time: the basic algorithm is much more active in the beginning that when it accumulates a lot of processing time; while Safra causes a lot of idle time at the end when all processes wait for their predecessor to pass on the token.

To conclude, the experiments confirm that the message complexity of SafraFT remains as for the fault sensitive version but its higher bit complexity causes a higher time complexity which leads to a later termination detection. 
Still, SafraFT causes only a moderate processing time overhead.

% TODO move to discussion
% TODO find convincing example of such an basic algorithm.
Our setup of an basic algorithm completing its work relatively fast and termination detection takes more time afterwards clearly shows a drawback of fault tolerant Safra.
However, a system with a long running basic algorithm e.g. multiple hours, would put this times in a whole different perspective.
Then the seconds taken to detect termination would be less of an issue and the moderate processing time overhead demostrated far more important.


% TODO not most of processing, total time is spent after termination just slightly half of it
\subsection{Influence of faults}
In the following paragraphs, I present and explain the data generated by runs under the presence of node crashes.
I run to really different scenarios: one considering networks with 1 to 5 nodes failing and one with 90\% of all nodes crashing.
These scenarios are chosen to show SafraFT in both the realistic case of a low number of faults to handle, as well as, the an extreme case; with the aim to confirm that SafraFT handles both cases correctly and
without unreasonable deterioration in any metric.

\subsubsection{Tokens}
For both the networks where between 1 and 5 nodes failed, as well as, for the highly faulty runs with 90\% node failure, the number of tokens increased compared to runs without any faults.
More so highly faulty networks more faults. The data is presented \cref{fig:TODO} and \cref{table:TODO}. % Table presents comparision with 0 configuration

Otherwise, the two types of fault simulation had a highly different influence on the tokens and tokens after termination metrics.

Between 1 and 5 node failures lead to a strong increase in the variance of tokens sent, as well as, tokens sent after termintaion.
This seems reasonable as runs with failing node might lead to more different situations as runs without fails e.g. one failing node could easily cause an extra token when it leads to an backup token being issued and forwarded (this token is marked black until for a whole run), at the same time, a single failing node that is a leaf in a Chandy Misra sinke tree and that crashes just before the call of announce at the successor  causes no further activity.
The same example provides an idea why the variability increases in big networks. 
That is because one extra round in a big network has a much higher impact on the token count than in a small network.

A example crafted similiarly would explain the extremely high maximum of tokens sent and overlinear increase of the average number of tokens in networks with 2000 nodes for 1 to 5 nodes failing.

Other than networks with one to 5 failing nodes, networks with up to 90\% node failure lead to a similiar variance in tokens and tokens after termination as a fault free networks.
A likely explanation is that the low survival rate of 1 out of 10 leads to less different scenarios than 995 nodes surviving as in the fault scenario treated the last paragraphs.

Even though, only one 10th of the nodes survive to participate in the latter token rounds, the highly faulty networks produced more tokens than any other network of the same size.
That is most likely due to the high amount of backup tokens generated (also shown in \cref{fig:TODO})

Different from all other networks, highly faulty networks exhibit a much lower token to token after termination ratio caused by the low number of nodes alive in the last rounds.

\begin{table}
	\begin{tabular}{rrrrrr}%
		\toprule
		\multicolumn{1}{c}{Network} &
		\multicolumn{1}{c}{No faults} &
		\multicolumn{1}{c}{5n} &
		\multicolumn{1}{c}{$\Delta$} &
		\multicolumn{1}{c}{90\%} &
		\multicolumn{1}{c}{$\Delta$} \\
		\midrule
		\csvreader[head to column names]{figures/tokens-faulty.csv}{}% use head of csv as column names
		{\\\networkSize & \noFaults & \fiveN & \differenceFiveN & \ninety & \differenceNinety}
		\\\bottomrule
	\end{tabular}
	\caption{Tokens before and after termination (in braces). For different fault scenarios compared to fault-free networks.}
	\label{table:tokens-faulty}
\end{table}
    
\subsubsection{Backup tokens}
The average amount of backup tokens sent for either fault simulation and every network size is lower than the number of faults.
This is because SafraFT only issues backup tokens when the fault of its successor is detected via the fault detector but not if this fault is noticed by receiving a token.
That result is suprising because the simulated fault detector in my setup detects faults in a really timely manner.
Still, there are some runs were 1 to 5 nodes fail but no backup token is issued for all network sizes. 

The other extreme were more backup tokens are issued than faults occure exists as well.
This can be explained by my decission to have node failing after issuesing a backup token.
For example, nodes \co{A}, \co{B} and \co{C} follow each other in the ring, node \co{C} fails which is detected by \co{B} and a backup token is issued.
After, issueing the backup token \co{B} fails on detection \co{A} issues a backup token towards \co{C}.
Only then \co{A} detects the failing of \co{C} and issues a second backup token to its new successor.  


\subsubsection{Processing Time}
The observations of this chapter are backed by \cref{table:processing-times-faulty}.

As for tokens, one can see an increase in processing times before termination under the presents of faults compared to fault free runs. 
This increase is stronger for runs with more faults.
Which is no surprise as these runs also produced more tokens.

For runs with 1 to 5 failing node a higher variability in the processing times before and after termination becomes apparent. 
A likely reasons for this has been explained in the \cref{ssec:tokens-faulty}.
As the processing time grows for this runs, so does the processing time after termination.

For highly faulty networks one observes results in line with the results for tokens in these networks.
The variability for processing time before or after termination is not raised compared to fault free networks.
The processing time taken is even higher than the one for less faulty networks.

Less processing time after termination is spent than for fault free networks because less tokens need to be send.
% TODO transpose these tables
\begin{table}
	\begin{tabular}{rrrrrr}%
		\toprule
		\multicolumn{1}{c}{Network} &
		\multicolumn{1}{c}{No faults} &
		\multicolumn{1}{c}{5n} &
		\multicolumn{1}{c}{$\Delta$} &
		\multicolumn{1}{c}{90\%} &
		\multicolumn{1}{c}{$\Delta$} \\
		\midrule
		\csvreader[head to column names]{figures/processing-times-faulty.csv}{}
		{\\\networkSize & \noFaults & \fiveN & \differenceFiveN & \ninety & \differenceNinety}
		\\\bottomrule
	\end{tabular}
	\caption{Processing times before and after termination (in braces) in seconds. For different fault scenarios compared to fault-free networks.}
	\label{table:processing-times-faulty}
\end{table}
    
 \subsection{Total time}
 In line with the observations from the two sections before, one observes:
 \begin{itemize}
 	\item an increase in total time spent which is more pronounced on more faulty networks
 	\item a higher variability for time spent before and after termination for less faulty networks
 	\item less time spent after termination by highly faulty networks
 \end{itemize}
 The absolute numbers and relative increases are presented in \cref{table:total-times-faulty}.
 
 \begin{table}
 	\begin{tabular}{rrrrrr}%
 		\toprule
 		\multicolumn{1}{c}{Network} &
 		\multicolumn{1}{c}{No faults} &
 		\multicolumn{1}{c}{5n} &
 		\multicolumn{1}{c}{$\Delta$} &
 		\multicolumn{1}{c}{90\%} &
 		\multicolumn{1}{c}{$\Delta$} \\
 		\midrule
 		\csvreader[head to column names]{figures/total-times-faulty.csv}{}
 		{\\\networkSize & \noFaults & \fiveN & \differenceFiveN & \ninety & \differenceNinety}
 		\\\bottomrule
 	\end{tabular}
 	\caption{Total times before and after termination (in braces) in seconds. For different fault scenarios compared to fault-free networks.}
 	\label{table:total-times-faulty}
 \end{table}
 
 % TODO why does total time after for less faulty networks not increase.







